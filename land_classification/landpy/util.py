import numpy as np
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import DataLoader
import torch
from .my_data_loader import MyDataLoader
from sklearn.metrics import jaccard_similarity_score as jsc
from sklearn.metrics import confusion_matrix  


def create_data_loaders(data_set, training_split, batch_size):
    '''
    Creates the train and validation loader to run the model

    :param data_set: Data set generated by the custom Data Loader Function
    :paramtype data_set: PyTorch DataLoader?

    :param training_split: Percentage of data that is used for training (remainder used for validation)
    :paramtype training_split: float

    :param batch_size: Batch size for the model
    :paramtype batch_size: int
    '''

    dataset_size = len(data_set)
    indices = list(range(dataset_size))
    split_1 = int(training_split * dataset_size)
    
    training_indices = indices[:split_1]
    validation_indices = indices[split_1:]
    
    train_sampler = SubsetRandomSampler(training_indices)
    validation_sampler = SubsetRandomSampler(validation_indices)
    
    train_loader = DataLoader(data_set, batch_size=batch_size, sampler=train_sampler,
        shuffle=False, pin_memory=True, drop_last=True)
    validation_loader = DataLoader(data_set, batch_size=batch_size, sampler=validation_sampler,
        shuffle=False, pin_memory=True, drop_last=True)

    return train_loader, validation_loader



def train_step(inputs, class_labels, optimizer, loss, unet):
    '''
    Completes the training step for a single batch

    ..param inputs: Training batch of satellite images
    ..paramtype: Torch Tensor

    ..param class_labels: Generated 1D image mask for coresponding satellite image
    ..paramtype: Torch Tensor

    ..param optimizer: Optimizer in use
    ..paramtype optimizer: Torch Optimizer

    ..param loss: Loss function used
    ..paramtype loss: Torch Loss

    ..param unet: The Model we are training
    ..paramtype unet: UNet custom class
    '''

    #zero out the gradients because we do not want them to accumulate on each step
    optimizer.zero_grad()
    outputs = unet(inputs)
    soft_max_output = torch.nn.LogSoftmax(dim=1)(outputs)
    loss_output = loss(soft_max_output, class_labels.long())
    pre_parameters = list(unet.parameters())[0].clone()
    loss_output.backward()
    optimizer.step()
    post_parameters = list(unet.parameters())[0].clone()

    if torch.equal(pre_parameters.data, post_parameters.data):
        print("No Update Occured")

    return loss_output


def mean_IOU(softmax_prediction_batch, actual_batch):
    '''
    Calculates the Mean Intersection over Union for a Batch

    ..param softmax_predicition_batch: The softmax output prediction from UNet Model
    ..paramtype softmax_prediction_batch: Torch Tensor

    ..param actual_batch: Actual hand labeled mask for evaluation
    ..paramtype actual_batch: Torch Tensor
    '''

    softmax_prediction = softmax_prediction_batch.cpu().numpy()
    prediction = np.argmax(softmax_prediction, axis=1)
    y_pred = prediction.flatten()

    y_true = actual_batch.cpu().numpy()
    y_true = y_true.flatten()

    current = confusion_matrix(y_true, y_pred, labels=[0,1,2,3,4,5])
    intersection = np.diag(current)
    ground_truth_set = current.sum(axis=1)
    predicted_set = current.sum(axis=0)
    union = ground_truth_set + predicted_set - intersection + 1e-8
    IoU = intersection / union.astype(np.float32)

    return np.mean(IoU)

def construct_image(prediction_label, label_mask_size):
    '''
    Takes in a prediction label and constructs a 3D RGB Image

    ..param prediction_label: The 2D prediction mask
    ..paramtype prediction_label: Numpy array

    ..param label_mask_size: The size of the mask label
    ..paramtype label_mask_size: int
    '''

    new_image = np.zeros((3,label_mask_size,label_mask_size))
    d = {
        0: (0,1,1),
        1: (1,1,0),
        2: (1,0,1),
        3: (0,1,0),
        4: (0,0,1),
        5: (1,1,1),
        6: (0,0,0)
    }
    for i in range(len(new_image[0])):
        for j in range(len(new_image[0])):
            class_ = prediction_label[i][j]
            scheme = d[class_]
            new_image[0][i][j] = scheme[0]
            new_image[1][i][j] = scheme[1]
            new_image[2][i][j] = scheme[2]
    return new_image


def calculate_output_size(input_size, first_kernel_size):
    '''
    Calculates the size of the label output due to model

    ..Future Work: Take in model as input to function and calculate using
        parameters taken from there

    ..Assumptions: 4 contracting blocks - bottleneck - 3 expansion
        ..Each contracting and expansion uses size 3 kernels except first
            kernel

    ..param input_size: Size of input image (assume square)
    ..paramtype input_size: int

    ..param first_kernel_size: Size of first convolutional kernel
    ..paramtype first_kernel_size: int

    ..return label_size: Label size output
    '''
    c_1 = input_size - ((first_kernel_size - 1)*2)
    mp_1 = c_1/2

    c_2 = mp_1 - 4
    mp_2 = c_2/2

    c_3 = mp_2 - 4
    mp_3 = c_3/2

    c_4 = mp_3 - 4
    mp_4 = c_4/2

    bottleneck = (mp_4 - 4)*2

    cat_1 = bottleneck
    e_1 = (cat_1 - 4)*2
    e_2 = (e_1 - 4)*2
    e_3 = (e_2 - 4)*2

    label_size = e_3 - 4

    return label_size


def validation_IOU(prediction_batch, actual_batch):
    '''
    NOT IN USE - mean_IOU is used
    '''
    numpy_output = prediction_batch.cpu().numpy()
    prediction = np.argmax(numpy_output, axis=1)
    labels = actual_batch.cpu().numpy()
    jsc_labels = labels.reshape(-1)
    jsc_outputs = prediction.reshape(-1)
    
    return jsc(jsc_outputs, jsc_labels)


